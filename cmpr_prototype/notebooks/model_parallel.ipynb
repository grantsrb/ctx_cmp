{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3913db94-e409-407e-a6e7-8a28b0ff6866",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GPT2Tokenizer, GPTJForCausalLM, DataCollatorWithPadding\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07766e72-08e3-400c-aed3-25c9b69365c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hf-internal-testing/tiny-random-gptj were not used when initializing GPTJForCausalLM: ['h.0.attn.k_proj.weight', 'h.0.mlp.fc_out.weight', 'h.4.attn.bias', 'wte.weight', 'h.0.mlp.fc_out.bias', 'h.2.mlp.fc_in.bias', 'h.3.attn.masked_bias', 'h.0.mlp.fc_in.bias', 'h.3.mlp.fc_out.weight', 'h.2.attn.q_proj.weight', 'h.2.attn.out_proj.weight', 'h.4.ln_1.weight', 'ln_f.weight', 'h.4.mlp.fc_out.bias', 'h.2.mlp.fc_out.weight', 'h.4.attn.q_proj.weight', 'h.4.attn.masked_bias', 'h.3.mlp.fc_out.bias', 'h.2.ln_1.weight', 'h.2.mlp.fc_out.bias', 'ln_f.bias', 'h.0.mlp.fc_in.weight', 'h.3.ln_1.bias', 'h.4.mlp.fc_in.weight', 'h.4.mlp.fc_in.bias', 'h.0.attn.q_proj.weight', 'h.4.mlp.fc_out.weight', 'h.1.mlp.fc_out.bias', 'h.1.attn.out_proj.weight', 'h.3.mlp.fc_in.bias', 'h.3.attn.k_proj.weight', 'h.4.ln_1.bias', 'h.0.attn.v_proj.weight', 'h.0.ln_1.bias', 'h.3.attn.bias', 'h.1.attn.v_proj.weight', 'h.4.attn.out_proj.weight', 'h.1.ln_1.weight', 'h.1.mlp.fc_in.weight', 'h.0.ln_1.weight', 'h.1.attn.masked_bias', 'h.1.mlp.fc_in.bias', 'h.0.attn.bias', 'h.3.attn.out_proj.weight', 'h.2.attn.v_proj.weight', 'h.0.attn.masked_bias', 'h.4.attn.k_proj.weight', 'h.3.attn.v_proj.weight', 'h.2.mlp.fc_in.weight', 'h.3.attn.q_proj.weight', 'score.weight', 'h.1.attn.bias', 'h.1.mlp.fc_out.weight', 'h.2.ln_1.bias', 'h.2.attn.masked_bias', 'h.2.attn.bias', 'h.1.ln_1.bias', 'h.1.attn.q_proj.weight', 'h.3.ln_1.weight', 'h.2.attn.k_proj.weight', 'h.0.attn.out_proj.weight', 'h.4.attn.v_proj.weight', 'h.1.attn.k_proj.weight', 'h.3.mlp.fc_in.weight']\n",
      "- This IS expected if you are initializing GPTJForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPTJForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_type = \"hf-internal-testing/tiny-random-gptj\"\n",
    "#model_type = \"gpt2-medium\"\n",
    "tokenizer =   AutoTokenizer.from_pretrained(model_type)\n",
    "\n",
    "hface_model = AutoModelForCausalLM.from_pretrained(model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a0573a3-8595-4f82-9995-92713bcf91d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hface_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "284f1575-6b0f-4e65-a5f6-775c93e24def",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hface_model.parallelize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cfa8e1b-3670-45f7-ad12-149e4ceae4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hface_model.deparallelize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b193150-060c-4522-af88-9de5f1ac3088",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ignore this for now\n",
    "#small_dataset = {\n",
    "#    \"rand\": {\n",
    "#        \"prompts\": [\n",
    "#            \"he's bald, he's mauled, he's called grindlewald... or so I'm tald.\",\n",
    "#            \"To win, you must move forward at all costs. There is no retreat. I get knocked down, but I get up again. You will never keep me down.\",\n",
    "#            \"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam\", \n",
    "#            \"quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum\",\n",
    "#            \"dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\",\n",
    "#        ],\n",
    "#        \"answers\": [\n",
    "#            \"But why does the pilgrim seek refuge?\",\n",
    "#            \"Backoff, headstrong\",\n",
    "#            \"Sed ut perspiciatis unde omnis iste natus error\",\n",
    "#            \"sit voluptatem accusantium doloremque laudantium\",\n",
    "#            \"totam rem aperiam, eaque ipsa quae ab illo inventore\",\n",
    "#        ]\n",
    "#    },\n",
    "#    \"blicket_direct\": {\n",
    "#        \"prompts\": [\n",
    "#            \"You have two items. At least one is a blicket. If an item touches a machine and it turns on, the item is a blicket. You take one of the items and place it on a machine. The machine does not turn on. Is the item a blicket?\",\n",
    "#            \"You have two items. At least one is a blicket. If an item touches a machine and it turns on, the item is a blicket. You take one of the items and place it on a machine. The machine turns on. Is the item a blicket?\",\n",
    "#            \"You have two tools. At least one of the tools is magic. If a tool is magic, it can fix anything. If a tool is not magic, it cannot fix anything. You take one of the tools and try to fix a washing machine. You fail to fix it. Is the tool you used magic?\",\n",
    "#            \"You have two tools. At least one of the tools is magic. If a tool is magic, it can fix anything. If a tool is not magic, it cannot fix anything. You take one of the tools and try to fix a washing machine. You fix it! Is the tool you used magic?\",\n",
    "#            \"I see four eggs sitting on a table. The waiter told me that at least 3 of the eggs are hard boiled. If an egg is hard boiled, it will spin evenly. If it is not, it will spin wobbly. I see the waiter take one of the eggs and spin it. The egg spins wobbly. Is that egg hardboiled?\",\n",
    "#            \"I see four eggs sitting on a table. The waiter told me that at least 3 of the eggs are hard boiled. If an egg is hard boiled, it will spin evenly. If it is not, it will spin wobbly. I see the waiter take one of the eggs and spin it. The egg spins evenly. Is that egg hardboiled?\",\n",
    "#        ],\n",
    "#        \"answers\": [\n",
    "#            \"no, it is not\",\n",
    "#            \"yes, it is\",\n",
    "#            \"no, it is not\",\n",
    "#            \"yes, it is\",\n",
    "#            \"no, it is not\",\n",
    "#            \"yes, it is\",\n",
    "#        ]\n",
    "#    },\n",
    "#    \"blicket_implicit\": {\n",
    "#        \"prompts\": [\n",
    "#            \"You have two items. At least one is a blicket. If an item touches a machine and the machine turns on, the item is a blicket. You take one of the items and place it on a machine. The machine does not turn on. Is the other item a blicket?\",\n",
    "#            \"You have two items. At least one is a blicket. If an item touches a machine and the machine turns on, the item is a blicket. You take one of the items and place it on a machine. The machine turns on. Is the other item a blicket?\",\n",
    "#            \"You have two items. Only one is a blicket. If an item touches a machine and it turns on, the item is a blicket. You take one of the items and place it on a machine. The machine turns on. Is the other item a blicket?\",\n",
    "#            \n",
    "#            \"You have two tools. At least one of the tools is magic. If a tool is magic, it can fix anything. If a tool is not magic, it cannot fix anything. You take one of the tools and try to fix a washing machine. You fail to fix it. Is the tool you did not use magic?\",\n",
    "#            \"You have two tools. At least one of the tools is magic. If a tool is magic, it can fix anything. If a tool is not magic, it cannot fix anything. You take one of the tools and try to fix a washing machine. You fix it! Is the tool that you did not use magic?\",\n",
    "#            \"You have two tools. Only one of the tools is magic. If a tool is magic, it can fix anything. If a tool is not magic, it cannot fix anything. You take one of the tools and try to fix a washing machine. You fix it! Is the tool that you did not use magic?\",\n",
    "#            \n",
    "#            \"I see four eggs sitting on a table. The waiter told me that at least 3 of the eggs are hard boiled. If an egg is hard boiled, it will spin evenly. If it is not, it will spin wobbly. I see the waiter take one of the eggs and spin it. The egg spins wobbly. Are the other eggs all hardboiled?\",\n",
    "#            \"I see four eggs sitting on a table. The waiter told me that at least 3 of the eggs are hard boiled. If an egg is hard boiled, it will spin evenly. If it is not, it will spin wobbly. I see the waiter take one of the eggs and spin it. The egg spins evenly. Are the other eggs all hardboiled?\",\n",
    "#            \"I see four eggs sitting on a table. The waiter told me that at only 3 of the eggs are hard boiled. If an egg is hard boiled, it will spin evenly. If it is not, it will spin wobbly. I see the waiter take one of the eggs and spin it. The egg spins evenly. Are the other eggs all hardboiled?\",\n",
    "#        ],\n",
    "#        \"answers\": [\n",
    "#            \"yes, it is\",\n",
    "#            \"we cannot know the answer from the given information\",\n",
    "#            \"no, it is not\",\n",
    "#            \n",
    "#            \"yes, it is\",\n",
    "#            \"we cannot know the answer from the given information\",\n",
    "#            \"no, it is not\",\n",
    "#            \n",
    "#            \"yes, they are\",\n",
    "#            \"we cannot know the answer from the given information\",\n",
    "#            \"no, they are not\",\n",
    "#        ]\n",
    "#    }\n",
    "#}\n",
    "#\n",
    "#string = small_dataset[\"rand\"][\"prompts\"][0]\n",
    "#string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e3e3dd6-73c3-47eb-b76e-0190f413a017",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
    "#\n",
    "#model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931728e2-f3d0-47f2-8496-9634d6b02ccb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8439d3b5-8b17-4d5b-9816-0eae65960a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = 100\n",
    "bsize = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "22a4098d-f972-4421-8217-97a99104e594",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (/home/grantsrb/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "    num_rows: 3668\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"glue\", \"mrpc\", split=\"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2146e353-879f-4ed0-8a04-0d0288b22f38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2TokenizerFast(name_or_path='hf-internal-testing/tiny-random-gptj', vocab_size=1000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0ab4aa8e-4d48-45e6-bbc3-d591130f9e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_tokens = {\n",
    "#    #\"pad_token\": \"<PAD>\",\n",
    "#    \"cls_token\": \"<CLS>\", # Using CLS token as compression token\n",
    "#}\n",
    "RMB = \"<RMB>\"\n",
    "CMP = \"<CMP>\"\n",
    "new_tokens = [RMB, CMP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4da86b81-3142-487a-8f66-7799224e3c3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_added = tokenizer.add_tokens(ordinary_tokens)\n",
    "num_added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "01ed7913-189d-44c7-be29-0fe4457f5219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2ec0cb6b-f94c-4c7f-b4ae-3c75987a3fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<RMB> [1000]\n",
      "<CMP> [1001]\n"
     ]
    }
   ],
   "source": [
    "for tok in ordinary_tokens:\n",
    "    print(tok, tokenizer.encode(tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "764dcf59-d118-4d8f-add8-914f6b06ad8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1002"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n,h = hface_model.transformer.wte.weight.shape\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e9376f41-9248-433c-bc0a-ddacdbc9ec9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(1002, 32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hface_model.resize_token_embeddings(n+num_added)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d1a5c044-0bfa-49b0-8874-f61fa248c6c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'heyo bo<RMB>'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"heyo bo\"\n",
    "x = s[:100] + new_tokens[0]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8d572ece-a2f9-4e3b-aa6f-f01b0e1c1af4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "976fab312b2f41618625c4c8243674a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_516181/680677656.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m     }\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   2813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2814\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnum_proc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mnum_proc\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2815\u001b[0;31m             return self._map_single(\n\u001b[0m\u001b[1;32m   2816\u001b[0m                 \u001b[0mfunction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2817\u001b[0m                 \u001b[0mwith_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwith_indices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Dataset\"\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"self\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0;31m# apply actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 546\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DatasetDict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    547\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    511\u001b[0m         }\n\u001b[1;32m    512\u001b[0m         \u001b[0;31m# apply actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 513\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DatasetDict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    514\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0;31m# re-apply format to the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/datasets/fingerprint.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m             \u001b[0;31m# Call actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m             \u001b[0;31m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m_map_single\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, disable_tqdm, desc, cache_only)\u001b[0m\n\u001b[1;32m   3234\u001b[0m                         )  # Something simpler?\n\u001b[1;32m   3235\u001b[0m                         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3236\u001b[0;31m                             batch = apply_function_on_filtered_inputs(\n\u001b[0m\u001b[1;32m   3237\u001b[0m                                 \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3238\u001b[0m                                 \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mapply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3110\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwith_rank\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3111\u001b[0m                 \u001b[0madditional_args\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3112\u001b[0;31m             \u001b[0mprocessed_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0madditional_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3113\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLazyDict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3114\u001b[0m                 processed_inputs = {\n",
      "\u001b[0;32m/tmp/ipykernel_516181/680677656.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(examples)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0msent1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mCMP\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sentence1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0minpts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"max_length\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_SEQ_LEN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0msent2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_token\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sentence2\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"max_length\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_SEQ_LEN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2521\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_target_context_manager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2522\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_input_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2523\u001b[0;31m             \u001b[0mencodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mall_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2524\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtext_target\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2525\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_target_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2607\u001b[0m                 )\n\u001b[1;32m   2608\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2609\u001b[0;31m             return self.batch_encode_plus(\n\u001b[0m\u001b[1;32m   2610\u001b[0m                 \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2611\u001b[0m                 \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mbatch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2790\u001b[0m         \u001b[0;31m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2791\u001b[0;31m         padding_strategy, truncation_strategy, max_length, kwargs = self._get_padding_truncation_strategies(\n\u001b[0m\u001b[1;32m   2792\u001b[0m             \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2793\u001b[0m             \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtruncation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_get_padding_truncation_strategies\u001b[0;34m(self, padding, truncation, max_length, pad_to_multiple_of, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2426\u001b[0m         \u001b[0;31m# Test if we have a padding token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2427\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpadding_strategy\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mPaddingStrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDO_NOT_PAD\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token_id\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2428\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   2429\u001b[0m                 \u001b[0;34m\"Asking to pad but the tokenizer does not have a padding token. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2430\u001b[0m                 \u001b[0;34m\"Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`."
     ]
    }
   ],
   "source": [
    "def encode(examples):\n",
    "    sent1 = [s + CMP for s in examples[\"sentence1\"]]\n",
    "    inpts = tokenizer(sent1,padding=\"max_length\", max_length=MAX_SEQ_LEN, truncation=True, return_tensors=\"pt\")\n",
    "    sent2 = [s + tokenizer.eos_token for s in examples[\"sentence2\"]]\n",
    "    outs = tokenizer(sent2, padding=\"max_length\", max_length=MAX_SEQ_LEN, truncation=True , return_tensors=\"pt\")\n",
    "    \n",
    "    examples[\"label\"] = torch.LongTensor(examples[\"label\"])\n",
    "    idx = examples[\"label\"]==0\n",
    "    \n",
    "    outs[\"input_ids\"][idx] = inpts[\"input_ids\"][idx].clone()\n",
    "    outs[\"attention_mask\"][idx] = inpts[\"attention_mask\"][idx].clone()\n",
    "    outs[\"input_ids\"][outs[\"input_ids\"]==tokenizer.cls_token_id] = tokenizer.eos_token_id\n",
    "    \n",
    "    idx = (inpts[\"input_ids\"]==tokenizer.cls_token_id).float().reshape(len(idx),-1).sum(-1)\n",
    "    if idx.sum() != len(idx):\n",
    "        i = torch.argmax((idx==0).float())\n",
    "        inpts[\"input_ids\"][i,-1] = tokenizer.cls_token_id\n",
    "    idx = (outs[\"input_ids\"]==tokenizer.eos_token_id).float().reshape(len(examples[\"label\"]),-1).sum(-1)\n",
    "    if idx.sum() != len(examples[\"label\"]):\n",
    "        i = torch.argmax((idx==0).float())\n",
    "        outs[\"input_ids\"][i,-1] = tokenizer.eos_token_id\n",
    "    \n",
    "    #s = (inpts[\"input_ids\"]==tokenizer.cls_token_id).float().reshape(len(idx),-1).sum()\n",
    "    #if s != len(inpts[\"input_ids\"]):\n",
    "    #    print(\"CLS token id:\", tokenizer.cls_token_id)\n",
    "    #    print(\"Sum:\", s)\n",
    "    #    print(\"Len:\", len(inpts[\"input_ids\"]))\n",
    "    #    x = inpts[\"input_ids\"]\n",
    "    #    idx = (x==tokenizer.cls_token_id).float().reshape(x.shape[0],-1).sum(-1)\n",
    "    #    idx = torch.argmax((idx==0).float())\n",
    "    #    print(\"idx:\", idx)\n",
    "    #    print(sent1[idx])\n",
    "    #    print(inpts[\"input_ids\"][idx])\n",
    "    #    print()\n",
    "    #    print()\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\":        inpts[\"input_ids\"],\n",
    "        \"attention_mask\":   inpts[\"attention_mask\"],\n",
    "        \"output_ids\":       outs[\"input_ids\"],\n",
    "        \"output_attn_mask\": outs[\"attention_mask\"],\n",
    "        \"labels\":           examples[\"label\"],\n",
    "    }\n",
    "    \n",
    "dataset = dataset.map(encode, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e34b702-5c35-420b-9100-44ee533460c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask', 'output_ids', 'output_attn_mask', 'labels'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d48426f-b51e-4413-904d-58a670d43a48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf5b4cbf-a41d-4877-8639-7e6580960888",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset[\"output_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea7816f6-8d0f-49dd-af78-8a8821ae3f08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'output_ids', 'output_attn_mask', 'labels'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.remove_columns([\"sentence1\", \"sentence2\", \"idx\", \"label\"])\n",
    "dataset[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f2d5d9a-2444-461b-bcdb-e7cb24ee310a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.set_format(type=\"torch\")\n",
    "#dataset.set_format(type=\"torch\")\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=bsize, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e6ea042-9f30-4528-a4af-aa557f27bbc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  464,  1705,  1908, 28289,  7303,   319,   257, 11626,   287,   706,\n",
      "           12, 24425,  7313,   764, 50258, 50257, 50257, 50257, 50257, 50257,\n",
      "        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
      "        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
      "        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
      "        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
      "        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
      "        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
      "        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
      "        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257])\n",
      "tensor([  464,  1705,  1908, 28289,  7303,   319,   257, 11626,   287,   706,\n",
      "           12, 24425,  7313,   764, 50256, 50257, 50257, 50257, 50257, 50257,\n",
      "        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
      "        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
      "        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
      "        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
      "        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
      "        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
      "        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
      "        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257])\n"
     ]
    }
   ],
   "source": [
    "data = next(iter(dataloader))\n",
    "idx = data[\"labels\"]==0\n",
    "print(data[\"input_ids\"][idx][0])\n",
    "print(data[\"output_ids\"][idx][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a8a0c0-6a1e-42f1-972a-0c9666f2bb9b",
   "metadata": {},
   "source": [
    "## Data Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9a3380d4-361d-4ce6-9ca4-8d8149b902f7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50259, 1024)\n",
       "    (wpe): Embedding(1024, 1024)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (12): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (13): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (14): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (15): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (16): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (17): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (18): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (19): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (20): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (21): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (22): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (23): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50259, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hface_model.to(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4d27c250-9e08-407c-8245-4c83041751bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(dataloader))\n",
    "device = next(hface_model.parameters()).get_device()\n",
    "with torch.no_grad():\n",
    "    inpts = {\n",
    "        \"input_ids\": data[\"input_ids\"].to(device),\n",
    "        \"attention_mask\": data[\"attention_mask\"].to(device),\n",
    "    }\n",
    "    outputs = hface_model.transformer(**inpts)\n",
    "    #logits = outputs.logits\n",
    "    #print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ef602f2d-de5c-42d5-8c7b-8973a91c5291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['last_hidden_state', 'past_key_values'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4a0b1516-2ddf-4c1f-80ba-bf9f4bca60fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 100, 1024])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[\"last_hidden_state\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d02cffb0-4add-4d08-a792-c654936cef04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outputs[\"past_key_values\"]) # Number of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5a065f25-fa43-47ff-8a81-a596b1a8bcd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 16, 100, 64])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[\"past_key_values\"][0][0].shape # Keys of first layer (or values, I'm not sure which, but probably keys because of naming key_values order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8b858f69-3bec-46f8-ad93-6633a625dede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.transformer(inputs_embeds=outputs[\"last_hidden_state\"], attention_mask=inpts[\"attention_mask\"]) # Use this to argue embeddings instead of ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "92c13ca2-32df-4d4e-be45-117fc3e20a07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 0])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"labels\"][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae91e0c-29c6-4c3e-91ef-0e42ffff2e39",
   "metadata": {},
   "source": [
    "Positive Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9a41a259-9652-44ea-aa27-00b3d459f6cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nursing schools turned away more than 5,000 qualified applicants in the past year because of shortages of faculty and classroom space.<CLS><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(data[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "09f7e54b-0b94-4428-bdcc-4267924bbe64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nursing schools turned away more than 5,000 qualified applicants in the past year because of shortages of faculty and classroom space.<|endoftext|><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(data[\"output_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e8f1c2-125e-40e5-a28e-268a3d9dfdc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "533641e8-2780-4a4c-92a1-2220faf95bcc",
   "metadata": {},
   "source": [
    "\n",
    "Negative Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fffdd09e-33db-4f18-a003-ce80d68968d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Java Enterprise System bundles a slew of Sun software for a yearly subscription of $ 100 per employee.<CLS><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(data[\"input_ids\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5b5fad74-7838-4034-b246-3fc370e7d81a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Java Enterprise System bundles a slew of Sun software for a yearly subscription of $ 100 per employee.<|endoftext|><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(data[\"output_ids\"][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4432791c-40a4-482a-941c-0f7f41895c0a",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fabc6aa6-abfd-4697-bd47-63b12585ee0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceAutoEncoder(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Trains a new token type to compress a sentence into a single vector representation\n",
    "    \"\"\"\n",
    "    def __init__(self, model, tokenizer, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        model: hugging face transformer model\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.hface_model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.CLS_ID = tokenizer.cls_token_id\n",
    "        self.CLS = tokenizer.cls_token\n",
    "        self.EOS_ID = tokenizer.eos_token_id\n",
    "        self.EOS = tokenizer.eos_token\n",
    "        \n",
    "    def forward(self, data):\n",
    "        \"\"\"\n",
    "        data: dict\n",
    "            \"input_ids\": LongTensor (B,S1)\n",
    "                the token indices of the input sequence. The CLS token should be appended to the end of each sentence.\n",
    "            \"attention_mask\": LongTensor (B,S1)\n",
    "                attention mask for padding purposes. 0s mean padding.\n",
    "            \"output_ids\": LongTensor (B,S2)\n",
    "                the token indices of the target sequence. An EOS token should be appended to the end of each sentence\n",
    "            \"output_attn_mask\": LongTensor (B,S2)\n",
    "                attention mask for padding purposes. 0s mean padding.\n",
    "        \"\"\"\n",
    "        \n",
    "        model = self.hface_model\n",
    "        inpt_embs = model.transformer.wte(data[\"input_ids\"]).data\n",
    "        idx = data[\"input_ids\"]==self.CLS_ID\n",
    "        inpt_embs[idx] = 0\n",
    "        inpt_embs[idx] += model.transformer.wte.weight[self.CLS_ID]\n",
    "        out_embs =  model.transformer.wte(data[\"output_ids\"]).data\n",
    "        \n",
    "        \n",
    "        fx = model.transformer(inputs_embeds=inpt_embs, attention_mask=data[\"attention_mask\"])\n",
    "        fx = fx[\"last_hidden_state\"][idx][:,None]\n",
    "        \n",
    "        \n",
    "        # Concat compressed representation to beginning of sentence\n",
    "        attn = torch.cat([torch.ones_like(data[\"output_attn_mask\"][:,:1]), data[\"output_attn_mask\"]], dim=1)\n",
    "        #attn = torch.pad(data[\"output_attn_mask\"], (1,0))\n",
    "        try:\n",
    "            out_embs = torch.cat([fx,out_embs], dim=1)\n",
    "            attn = torch.cat([torch.ones_like(data[\"output_attn_mask\"][:,:1]), data[\"output_attn_mask\"]], dim=1)\n",
    "        except:\n",
    "            print(\"Data\")\n",
    "            for k in data: print(k, data[k].shape)\n",
    "            print(\"idx sum:\", idx.float().sum())\n",
    "            print(\"In Embds\", inpt_embs.shape)\n",
    "            print(\"Out Embds\", out_embs.shape)\n",
    "            print(\"FX\", fx.shape)\n",
    "            assert False\n",
    "        \n",
    "        preds = model(inputs_embeds=out_embs, attention_mask=attn).logits\n",
    "        return preds\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "88a1c8d9-fb75-4543-8f65-9319b9424808",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceAutoEncoder(hface_model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432c9959-6a37-421c-ad6e-cc51248c58e6",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "17624719-7c0e-4909-8675-0f50d0a96c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-4\n",
    "l2 = 1e-3\n",
    "n_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "483530ff-8e15-4f87-ae88-bbd14d95b376",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = model.hface_model.transformer.wte.parameters()\n",
    "optimizer = torch.optim.Adam(params, lr=lr, weight_decay=l2)\n",
    "loss_fxn = torch.nn.CrossEntropyLoss()\n",
    "params = set(params)\n",
    "for p in model.parameters():\n",
    "    if p not in params: p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "26ec7221-89f4-458c-975b-f718d5e1c8ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Beginning Epoch 0\n",
      "Loss 84.24694 -- Acc: 0.0\n",
      "Loss 81.21325 -- Acc: 0.0\n",
      "Loss 70.53687 -- Acc: 0.0\n",
      "Loss 75.70064 -- Acc: 0.0\n",
      "Loss 89.97944 -- Acc: 0.0\n",
      "Loss 63.91817 -- Acc: 0.0\n",
      "Loss 67.60391 -- Acc: 0.0\n",
      "Loss 79.08193 -- Acc: 0.0\n",
      "Loss 75.36988 -- Acc: 0.0\n",
      "Loss 87.33172 -- Acc: 0.0\n",
      "Loss 78.30166 -- Acc: 0.0\n",
      "Loss 68.78848 -- Acc: 0.0\n",
      "Loss 78.52134 -- Acc: 0.0\n",
      "Loss 69.13879 -- Acc: 0.0\n",
      "Loss 77.55251 -- Acc: 0.0\n",
      "Loss 69.66534 -- Acc: 0.0\n",
      "Loss 84.58016 -- Acc: 0.0\n",
      "Loss 76.60291 -- Acc: 0.0\n",
      "Loss 87.75251 -- Acc: 0.0\n",
      "Loss 82.21988 -- Acc: 0.0\n",
      "Loss 67.91528 -- Acc: 0.0\n",
      "Loss 83.62623 -- Acc: 0.0\n",
      "Loss 93.74638 -- Acc: 0.0\n",
      "Avg Loss: 17799.98936 -- Avg Acc: 0.0\n",
      "\n",
      "\n",
      "Beginning Epoch 1\n",
      "Loss 79.82491 -- Acc: 0.0\n",
      "Loss 100.64024 -- Acc: 0.0\n",
      "Loss 70.68429 -- Acc: 0.0\n",
      "Loss 76.25298 -- Acc: 0.0\n",
      "Loss 76.36373 -- Acc: 0.0\n",
      "Loss 75.4203 -- Acc: 0.0\n",
      "Loss 70.43633 -- Acc: 0.0\n",
      "Loss 66.94095 -- Acc: 0.0\n",
      "Loss 60.8101 -- Acc: 0.0\n",
      "Loss 79.07037 -- Acc: 0.0\n",
      "Loss 80.07519 -- Acc: 0.0\n",
      "Loss 74.47204 -- Acc: 0.0\n",
      "Loss 64.73815 -- Acc: 0.0\n",
      "Loss 79.96474 -- Acc: 0.0\n",
      "Loss 75.52843 -- Acc: 0.0\n",
      "Loss 70.97234 -- Acc: 0.0\n",
      "Loss 75.6472 -- Acc: 0.0\n",
      "Loss 87.69759 -- Acc: 0.0\n",
      "Loss 82.31384 -- Acc: 0.0\n",
      "Loss 76.89906 -- Acc: 0.0\n",
      "Loss 77.68832 -- Acc: 0.0\n",
      "Loss 81.7454 -- Acc: 0.0\n",
      "Loss 80.3371 -- Acc: 0.0\n",
      "Avg Loss: 17919.70769 -- Avg Acc: 0.0\n",
      "\n",
      "\n",
      "Beginning Epoch 2\n",
      "Loss 79.46243 -- Acc: 0.0\n",
      "Loss 74.74407 -- Acc: 0.0\n",
      "Loss 73.89275 -- Acc: 0.0\n",
      "Loss 70.63107 -- Acc: 0.0\n",
      "Loss 83.493 -- Acc: 0.0\n",
      "Loss 80.90493 -- Acc: 0.0\n",
      "Loss 82.6437 -- Acc: 0.0\n",
      "Loss 76.53478 -- Acc: 0.0\n",
      "Loss 80.83591 -- Acc: 0.0\n",
      "Loss 82.3447 -- Acc: 0.0\n",
      "Loss 83.52557 -- Acc: 0.0\n",
      "Loss 92.09087 -- Acc: 0.0\n",
      "Loss 84.05543 -- Acc: 0.0\n",
      "Loss 70.77204 -- Acc: 0.0\n",
      "Loss 79.34556 -- Acc: 0.0\n",
      "Loss 76.24314 -- Acc: 0.0\n",
      "Loss 76.61724 -- Acc: 0.0\n",
      "Loss 77.60274 -- Acc: 0.0\n",
      "Loss 74.11074 -- Acc: 0.0\n",
      "Loss 73.75486 -- Acc: 0.0\n",
      "Loss 87.53244 -- Acc: 0.0\n",
      "Loss 66.51109 -- Acc: 0.0\n",
      "Loss 81.46381 -- Acc: 0.0\n",
      "Avg Loss: 17902.20019 -- Avg Acc: 0.0\n",
      "\n",
      "\n",
      "Beginning Epoch 3\n",
      "Loss 71.68752 -- Acc: 0.0\n",
      "Loss 64.25822 -- Acc: 0.0\n",
      "Loss 77.9677 -- Acc: 0.0\n",
      "Loss 89.55388 -- Acc: 0.0\n",
      "Loss 67.29787 -- Acc: 0.0\n",
      "Loss 81.32365 -- Acc: 0.0\n",
      "Loss 80.51591 -- Acc: 0.0\n",
      "Loss 83.12415 -- Acc: 0.0\n",
      "Loss 91.43855 -- Acc: 0.0\n",
      "Loss 73.78065 -- Acc: 0.0\n",
      "Loss 71.73784 -- Acc: 0.0\n",
      "Loss 82.63908 -- Acc: 0.0\n",
      "Loss 70.16866 -- Acc: 0.0\n",
      "Loss 75.53464 -- Acc: 0.0\n",
      "Loss 72.00228 -- Acc: 0.0\n",
      "Loss 74.83525 -- Acc: 0.0\n",
      "Loss 77.72122 -- Acc: 0.0\n",
      "Loss 79.69677 -- Acc: 0.0\n",
      "Loss 73.69691 -- Acc: 0.0\n",
      "Loss 68.87765 -- Acc: 0.0\n",
      "Loss 84.66753 -- Acc: 0.0\n",
      "Loss 70.47257 -- Acc: 0.0\n",
      "Loss 91.51109 -- Acc: 0.0\n",
      "Avg Loss: 17911.93253 -- Avg Acc: 0.01559\n",
      "\n",
      "\n",
      "Beginning Epoch 4\n",
      "Loss 89.99382 -- Acc: 0.0\n",
      "Loss 64.64054 -- Acc: 0.0\n",
      "Loss 71.21803 -- Acc: 0.0\n",
      "Loss 82.19976 -- Acc: 0.0\n",
      "Loss 81.47285 -- Acc: 0.0\n",
      "Loss 77.58747 -- Acc: 0.0\n",
      "Loss 74.37291 -- Acc: 0.0\n",
      "Loss 73.88466 -- Acc: 0.0\n",
      "Loss 72.91949 -- Acc: 0.0\n",
      "Loss 77.81999 -- Acc: 0.0\n",
      "Loss 76.7514 -- Acc: 0.0\n",
      "Loss 76.04317 -- Acc: 0.0\n",
      "Loss 78.73096 -- Acc: 0.0\n",
      "Loss 76.70979 -- Acc: 0.0\n",
      "Loss 73.79435 -- Acc: 0.0\n",
      "Loss 87.81569 -- Acc: 0.0\n",
      "Loss 72.05862 -- Acc: 0.0\n",
      "Loss 81.72597 -- Acc: 0.0\n",
      "Loss 77.5417 -- Acc: 0.0\n",
      "Loss 74.96561 -- Acc: 0.0\n",
      "Loss 80.21156 -- Acc: 0.0\n",
      "Loss 77.02017 -- Acc: 0.0\n",
      "Loss 77.86738 -- Acc: 0.0\n",
      "Avg Loss: 17692.45605 -- Avg Acc: 0.0\n",
      "\n",
      "\n",
      "Beginning Epoch 5\n",
      "Loss 66.29842 -- Acc: 0.0\n",
      "Loss 71.39176 -- Acc: 0.0\n",
      "Loss 79.83213 -- Acc: 0.0\n",
      "Loss 78.78168 -- Acc: 0.0\n",
      "Loss 80.24123 -- Acc: 0.0\n",
      "Loss 71.52975 -- Acc: 0.0\n",
      "Loss 71.6489 -- Acc: 0.0\n",
      "Loss 75.25779 -- Acc: 0.0\n",
      "Loss 73.03783 -- Acc: 0.0\n",
      "Loss 74.92722 -- Acc: 0.0\n",
      "Loss 74.13625 -- Acc: 0.0\n",
      "Loss 78.56201 -- Acc: 0.0\n",
      "Loss 78.29543 -- Acc: 0.0\n",
      "Loss 67.21171 -- Acc: 0.0\n",
      "Loss 77.58284 -- Acc: 0.0\n",
      "Loss 64.79234 -- Acc: 0.0\n",
      "Loss 70.75773 -- Acc: 0.0\n",
      "Loss 65.11437 -- Acc: 0.0\n",
      "Loss 65.26715 -- Acc: 0.0\n",
      "Loss 79.14594 -- Acc: 0.0\n",
      "Loss 74.33862 -- Acc: 0.0\n",
      "Loss 87.91499 -- Acc: 0.0\n",
      "Loss 81.99494 -- Acc: 0.0\n",
      "Avg Loss: 17793.65527 -- Avg Acc: 0.0\n",
      "\n",
      "\n",
      "Beginning Epoch 6\n",
      "Loss 80.08392 -- Acc: 0.0\n",
      "Loss 79.01215 -- Acc: 0.0\n",
      "Loss 68.07707 -- Acc: 0.0\n",
      "Loss 71.5682 -- Acc: 0.0\n",
      "Loss 95.42755 -- Acc: 0.0\n",
      "Loss 73.76999 -- Acc: 0.0\n",
      "Loss 76.48655 -- Acc: 0.0\n",
      "Loss 69.42309 -- Acc: 0.0\n",
      "Loss 82.90266 -- Acc: 0.0\n",
      "Loss 73.24828 -- Acc: 0.0\n",
      "Loss 78.16748 -- Acc: 0.0\n",
      "Loss 73.18501 -- Acc: 0.0\n",
      "Loss 84.20986 -- Acc: 0.0\n",
      "Loss 67.07353 -- Acc: 0.0\n",
      "Loss 79.58482 -- Acc: 0.0\n",
      "Loss 69.75226 -- Acc: 0.0\n",
      "Loss 74.8223 -- Acc: 0.0\n",
      "Loss 81.47073 -- Acc: 0.0\n",
      "Loss 77.24911 -- Acc: 0.0\n",
      "Loss 82.8879 -- Acc: 0.0\n",
      "Loss 75.82275 -- Acc: 0.0\n",
      "Loss 76.65005 -- Acc: 0.0\n",
      "Loss 74.14484 -- Acc: 0.0\n",
      "Avg Loss: 17807.49895 -- Avg Acc: 0.0\n",
      "\n",
      "\n",
      "Beginning Epoch 7\n",
      "Loss 77.91036 -- Acc: 0.0\n",
      "Loss 75.98425 -- Acc: 0.0\n",
      "Loss 70.21688 -- Acc: 0.0\n",
      "Loss 73.77699 -- Acc: 0.0\n",
      "Loss 70.82665 -- Acc: 0.0\n",
      "Loss 68.76666 -- Acc: 0.0\n",
      "Loss 78.029 -- Acc: 0.0\n",
      "Loss 70.97248 -- Acc: 0.0\n",
      "Loss 69.78043 -- Acc: 0.0\n",
      "Loss 78.81661 -- Acc: 0.0\n",
      "Loss 87.09967 -- Acc: 0.0\n",
      "Loss 72.13876 -- Acc: 0.0\n",
      "Loss 80.33731 -- Acc: 0.0\n",
      "Loss 88.19285 -- Acc: 0.0\n",
      "Loss 68.19586 -- Acc: 0.0\n",
      "Loss 75.47827 -- Acc: 0.0\n",
      "Loss 72.5958 -- Acc: 0.0\n",
      "Loss 71.59941 -- Acc: 0.0\n",
      "Loss 94.37206 -- Acc: 0.0\n",
      "Loss 79.20151 -- Acc: 0.0\n",
      "Loss 71.49739 -- Acc: 0.0\n",
      "Loss 70.11926 -- Acc: 0.0\n",
      "Loss 87.33758 -- Acc: 0.0\n",
      "Avg Loss: 17812.69658 -- Avg Acc: 0.0\n",
      "\n",
      "\n",
      "Beginning Epoch 8\n",
      "Loss 79.55695 -- Acc: 0.0\n",
      "Loss 67.65296 -- Acc: 0.0\n",
      "Loss 74.95978 -- Acc: 0.0\n",
      "Loss 86.8659 -- Acc: 0.0\n",
      "Loss 78.01847 -- Acc: 0.0\n",
      "Loss 64.04151 -- Acc: 0.0\n",
      "Loss 98.43933 -- Acc: 0.0\n",
      "Loss 75.43904 -- Acc: 0.0\n",
      "Loss 77.49982 -- Acc: 0.0\n",
      "Loss 72.5286 -- Acc: 0.0\n",
      "Loss 79.58165 -- Acc: 0.0\n",
      "Loss 91.31284 -- Acc: 0.0\n",
      "Loss 85.88102 -- Acc: 0.0\n",
      "Loss 64.51722 -- Acc: 0.0\n",
      "Loss 86.23406 -- Acc: 0.0\n",
      "Loss 67.73991 -- Acc: 0.0\n",
      "Loss 75.36188 -- Acc: 0.0\n",
      "Loss 82.17609 -- Acc: 0.0\n",
      "Loss 88.13156 -- Acc: 0.0\n",
      "Loss 73.29247 -- Acc: 0.0\n",
      "Loss 92.07194 -- Acc: 0.0\n",
      "Loss 72.94574 -- Acc: 0.0\n",
      "Loss 83.97109 -- Acc: 0.0\n",
      "Avg Loss: 17902.11973 -- Avg Acc: 0.0\n",
      "\n",
      "\n",
      "Beginning Epoch 9\n",
      "Loss 67.52357 -- Acc: 0.0\n",
      "Loss 74.20725 -- Acc: 0.0\n",
      "Loss 80.58025 -- Acc: 0.0\n",
      "Loss 71.60577 -- Acc: 0.0\n",
      "Loss 84.21326 -- Acc: 0.0\n",
      "Loss 89.79317 -- Acc: 0.0\n",
      "Loss 63.05757 -- Acc: 0.0\n",
      "Loss 93.82616 -- Acc: 0.0\n",
      "Loss 78.34867 -- Acc: 0.0\n",
      "Loss 75.62714 -- Acc: 0.0\n",
      "Loss 65.26359 -- Acc: 0.0\n",
      "Loss 75.82092 -- Acc: 0.0\n",
      "Loss 65.24634 -- Acc: 0.0\n",
      "Loss 77.80898 -- Acc: 0.0\n",
      "Loss 93.16597 -- Acc: 0.0\n",
      "Loss 86.47865 -- Acc: 0.0\n",
      "Loss 73.64124 -- Acc: 0.0\n",
      "Loss 85.17921 -- Acc: 0.0\n",
      "Loss 70.02602 -- Acc: 0.0\n",
      "Loss 80.43166 -- Acc: 0.0\n",
      "Loss 77.54025 -- Acc: 0.0\n",
      "Loss 74.47542 -- Acc: 0.0\n",
      "Loss 67.35141 -- Acc: 0.0\n",
      "Avg Loss: 17788.50324 -- Avg Acc: 0.0\n",
      "\n",
      "\n",
      "Beginning Epoch 10\n",
      "Loss 78.66319 -- Acc: 0.0\n",
      "Loss 70.3867 -- Acc: 0.0\n",
      "Loss 88.98766 -- Acc: 0.0\n",
      "Loss 81.62764 -- Acc: 0.0\n",
      "Loss 61.1457 -- Acc: 0.0\n",
      "Loss 77.63509 -- Acc: 0.0\n",
      "Loss 82.28296 -- Acc: 0.0\n",
      "Loss 68.85396 -- Acc: 0.0\n",
      "Loss 81.47432 -- Acc: 0.0\n",
      "Loss 80.13704 -- Acc: 0.0\n",
      "Loss 77.74348 -- Acc: 0.0\n",
      "Loss 71.67693 -- Acc: 0.0\n",
      "Loss 88.0944 -- Acc: 0.0\n",
      "Loss 69.45497 -- Acc: 0.0\n",
      "Loss 74.84196 -- Acc: 0.0\n",
      "Loss 76.53695 -- Acc: 0.0\n",
      "Loss 87.99831 -- Acc: 0.0\n",
      "Loss 80.81509 -- Acc: 0.0\n",
      "Loss 78.98633 -- Acc: 0.0\n",
      "Loss 77.60976 -- Acc: 0.0\n",
      "Loss 75.50365 -- Acc: 0.0\n",
      "Loss 81.85859 -- Acc: 0.0\n",
      "Loss 75.70416 -- Acc: 0.0\n",
      "Avg Loss: 17738.15502 -- Avg Acc: 0.0\n",
      "\n",
      "\n",
      "Beginning Epoch 11\n",
      "Loss 76.70713 -- Acc: 0.0\n",
      "Loss 69.55243 -- Acc: 0.0\n",
      "Loss 79.54372 -- Acc: 0.0\n",
      "Loss 75.36945 -- Acc: 0.0\n",
      "Loss 68.75067 -- Acc: 0.0\n",
      "Loss 71.64785 -- Acc: 0.0\n",
      "Loss 67.14252 -- Acc: 0.0\n",
      "Loss 82.18624 -- Acc: 0.0\n",
      "Loss 71.16064 -- Acc: 0.0\n",
      "Loss 83.41726 -- Acc: 0.0\n",
      "Loss 73.86781 -- Acc: 0.0\n",
      "Loss 98.03291 -- Acc: 0.0\n",
      "Loss 77.09337 -- Acc: 0.0\n",
      "Loss 81.18035 -- Acc: 0.0\n",
      "Loss 80.88119 -- Acc: 0.0\n",
      "Loss 76.03564 -- Acc: 0.0\n",
      "Loss 69.76603 -- Acc: 0.0\n",
      "Loss 78.86009 -- Acc: 0.0\n",
      "Loss 78.78604 -- Acc: 0.0\n",
      "Loss 73.0506 -- Acc: 0.0\n",
      "Loss 70.77438 -- Acc: 0.0\n",
      "Loss 79.74307 -- Acc: 0.0\n",
      "Loss 79.62006 -- Acc: 0.0\n",
      "Avg Loss: 17888.56934 -- Avg Acc: 0.0\n",
      "\n",
      "\n",
      "Beginning Epoch 12\n",
      "Loss 71.07205 -- Acc: 0.0\n",
      "Loss 81.89257 -- Acc: 0.0\n",
      "Loss 90.73472 -- Acc: 0.0\n",
      "Loss 82.40205 -- Acc: 0.0\n",
      "Loss 73.89553 -- Acc: 0.0\n",
      "Loss 71.15955 -- Acc: 0.0\n",
      "Loss 77.45953 -- Acc: 0.0\n",
      "Loss 76.0414 -- Acc: 0.0\n",
      "Loss 69.55229 -- Acc: 0.0\n",
      "Loss 77.57259 -- Acc: 0.0\n",
      "Loss 84.93983 -- Acc: 0.0\n",
      "Loss 77.14285 -- Acc: 0.0\n",
      "Loss 70.77631 -- Acc: 0.0\n",
      "Loss 72.88038 -- Acc: 0.0\n",
      "Loss 63.20393 -- Acc: 0.0\n",
      "Loss 70.89677 -- Acc: 0.0\n",
      "Loss 73.16733 -- Acc: 0.0\n",
      "Loss 80.5705 -- Acc: 0.0\n",
      "Loss 80.31297 -- Acc: 0.0\n",
      "Loss 85.68206 -- Acc: 0.0\n",
      "Loss 86.62289 -- Acc: 0.0\n",
      "Loss 81.21133 -- Acc: 0.0\n",
      "Loss 83.48573 -- Acc: 0.0\n",
      "Avg Loss: 17943.64626 -- Avg Acc: 0.0\n",
      "\n",
      "\n",
      "Beginning Epoch 13\n",
      "Loss 79.20739 -- Acc: 0.0\n",
      "Loss 85.08298 -- Acc: 0.0\n",
      "Loss 72.63215 -- Acc: 0.0\n",
      "Loss 71.07513 -- Acc: 0.0\n",
      "Loss 68.77039 -- Acc: 0.0\n",
      "Loss 79.9337 -- Acc: 0.0\n",
      "Loss 77.50568 -- Acc: 0.0\n",
      "Loss 90.73617 -- Acc: 0.0\n",
      "Loss 65.19499 -- Acc: 0.0\n",
      "Loss 73.75391 -- Acc: 0.0\n",
      "Loss 77.28013 -- Acc: 0.0\n",
      "Loss 60.62401 -- Acc: 0.0\n",
      "Loss 90.02521 -- Acc: 0.0\n",
      "Loss 79.43025 -- Acc: 0.0\n",
      "Loss 77.77505 -- Acc: 0.0\n",
      "Loss 85.16198 -- Acc: 0.0\n",
      "Loss 70.84267 -- Acc: 0.0\n",
      "Loss 72.87109 -- Acc: 0.0\n",
      "Loss 88.13622 -- Acc: 0.0\n",
      "Loss 93.9948 -- Acc: 0.0\n",
      "Loss 89.44343 -- Acc: 0.0\n",
      "Loss 87.15519 -- Acc: 0.0\n",
      "Loss 77.47636 -- Acc: 0.0\n",
      "Avg Loss: 17865.51857 -- Avg Acc: 0.0\n",
      "\n",
      "\n",
      "Beginning Epoch 14\n",
      "Loss 73.95692 -- Acc: 0.0\n",
      "Loss 77.881 -- Acc: 0.0\n",
      "Loss 86.8122 -- Acc: 0.0\n",
      "Loss 73.48433 -- Acc: 0.0\n",
      "Loss 77.91033 -- Acc: 0.0\n",
      "Loss 76.82246 -- Acc: 0.0\n",
      "Loss 82.90842 -- Acc: 0.0\n",
      "Loss 78.45257 -- Acc: 0.0\n",
      "Loss 76.87957 -- Acc: 0.0\n",
      "Loss 95.3147 -- Acc: 0.0\n",
      "Loss 83.35249 -- Acc: 0.0\n",
      "Loss 70.11699 -- Acc: 0.0\n",
      "Loss 78.93063 -- Acc: 0.0\n",
      "Loss 82.75419 -- Acc: 0.0\n",
      "Loss 78.53026 -- Acc: 0.0\n",
      "Loss 72.87962 -- Acc: 0.0\n",
      "Loss 85.51115 -- Acc: 0.0\n",
      "Loss 75.65963 -- Acc: 0.0\n",
      "Loss 82.96916 -- Acc: 0.0\n",
      "Loss 76.86315 -- Acc: 0.0\n",
      "Loss 84.47532 -- Acc: 0.0\n",
      "Loss 73.40185 -- Acc: 0.0\n",
      "Loss 78.57623 -- Acc: 0.0\n",
      "Avg Loss: 17963.63916 -- Avg Acc: 0.0\n",
      "\n",
      "\n",
      "Beginning Epoch 15\n",
      "Loss 88.72434 -- Acc: 0.0\n",
      "Loss 75.90163 -- Acc: 0.0\n",
      "Loss 64.65897 -- Acc: 0.0\n",
      "Loss 77.11843 -- Acc: 0.0\n",
      "Loss 77.03854 -- Acc: 0.0\n",
      "Loss 78.19308 -- Acc: 0.0\n",
      "Loss 76.31372 -- Acc: 0.0\n",
      "Loss 85.11214 -- Acc: 0.0\n",
      "Loss 87.16711 -- Acc: 0.0\n",
      "Loss 87.53346 -- Acc: 0.0\n",
      "Loss 70.92819 -- Acc: 0.0\n",
      "Loss 72.62985 -- Acc: 0.0\n",
      "Loss 91.31328 -- Acc: 0.0\n",
      "Loss 79.47762 -- Acc: 0.0\n",
      "Loss 68.89413 -- Acc: 0.0\n",
      "Loss 78.05842 -- Acc: 0.0\n",
      "Loss 75.02187 -- Acc: 0.0\n",
      "Loss 85.40388 -- Acc: 0.0\n",
      "Loss 77.13922 -- Acc: 0.0\n",
      "Loss 75.80865 -- Acc: 0.0\n",
      "Loss 76.87533 -- Acc: 0.0\n",
      "Loss 78.82793 -- Acc: 0.0\n",
      "Loss 79.03951 -- Acc: 0.0\n",
      "Avg Loss: 17867.82356 -- Avg Acc: 0.0\n",
      "\n",
      "\n",
      "Beginning Epoch 16\n",
      "Loss 93.29803 -- Acc: 0.0\n",
      "Loss 67.52414 -- Acc: 0.0\n",
      "Loss 79.52371 -- Acc: 0.0\n",
      "Loss 72.36575 -- Acc: 0.0\n",
      "Loss 66.65192 -- Acc: 0.0\n",
      "Loss 73.86375 -- Acc: 0.0\n",
      "Loss 65.90156 -- Acc: 0.0\n",
      "Loss 83.66386 -- Acc: 0.0\n",
      "Loss 77.37302 -- Acc: 0.0\n",
      "Loss 73.74535 -- Acc: 0.0\n",
      "Loss 91.65024 -- Acc: 0.0\n",
      "Loss 84.59503 -- Acc: 0.0\n",
      "Loss 81.45345 -- Acc: 0.0\n",
      "Loss 71.33712 -- Acc: 0.0\n",
      "Loss 90.48661 -- Acc: 0.0\n",
      "Loss 74.38294 -- Acc: 0.0\n",
      "Loss 74.28556 -- Acc: 0.0\n",
      "Loss 71.64837 -- Acc: 0.0\n",
      "Loss 80.837 -- Acc: 0.0\n",
      "Loss 82.93456 -- Acc: 0.0\n",
      "Loss 69.4968 -- Acc: 0.0\n",
      "Loss 81.9731 -- Acc: 0.0\n",
      "Loss 76.87854 -- Acc: 0.0\n",
      "Avg Loss: 17752.36316 -- Avg Acc: 0.0\n",
      "\n",
      "\n",
      "Beginning Epoch 17\n",
      "Loss 73.48279 -- Acc: 0.0\n",
      "Loss 70.48515 -- Acc: 0.0\n",
      "Loss 70.81384 -- Acc: 0.0\n",
      "Loss 83.92499 -- Acc: 0.0\n",
      "Loss 76.33888 -- Acc: 0.0\n",
      "Loss 70.40997 -- Acc: 0.0\n",
      "Loss 80.50063 -- Acc: 0.0\n",
      "Loss 76.8102 -- Acc: 0.0\n",
      "Loss 68.85782 -- Acc: 0.0\n",
      "Loss 93.72132 -- Acc: 0.0\n",
      "Loss 82.09161 -- Acc: 0.0\n",
      "Loss 69.18732 -- Acc: 0.0\n",
      "Loss 67.35841 -- Acc: 0.0\n",
      "Loss 71.56404 -- Acc: 0.0\n",
      "Loss 66.00596 -- Acc: 0.0\n",
      "Loss 65.2266 -- Acc: 0.0\n",
      "Loss 79.27386 -- Acc: 0.0\n",
      "Loss 98.89841 -- Acc: 0.0\n",
      "Loss 68.87977 -- Acc: 0.0\n",
      "Loss 70.2458 -- Acc: 0.0\n",
      "Loss 72.09175 -- Acc: 0.0\n",
      "Loss 80.37157 -- Acc: 0.0\n",
      "Loss 75.67381 -- Acc: 0.0\n",
      "Avg Loss: 17817.07787 -- Avg Acc: 0.0\n",
      "\n",
      "\n",
      "Beginning Epoch 18\n",
      "Loss 91.36996 -- Acc: 0.0\n",
      "Loss 80.69952 -- Acc: 0.0\n",
      "Loss 79.94895 -- Acc: 0.0\n",
      "Loss 71.0371 -- Acc: 0.0\n",
      "Loss 85.50533 -- Acc: 0.0\n",
      "Loss 63.75201 -- Acc: 0.0\n",
      "Loss 74.33851 -- Acc: 0.0\n",
      "Loss 70.83998 -- Acc: 0.0\n",
      "Loss 77.59373 -- Acc: 0.0\n",
      "Loss 82.70709 -- Acc: 0.0\n",
      "Loss 85.43309 -- Acc: 0.0\n",
      "Loss 77.75404 -- Acc: 0.0\n",
      "Loss 81.12952 -- Acc: 0.0\n",
      "Loss 77.39977 -- Acc: 0.0\n",
      "Loss 78.94788 -- Acc: 0.0\n",
      "Loss 93.88282 -- Acc: 0.0\n",
      "Loss 88.05538 -- Acc: 0.0\n",
      "Loss 68.91454 -- Acc: 0.0\n",
      "Loss 84.91 -- Acc: 0.0\n",
      "Loss 62.94938 -- Acc: 0.0\n",
      "Loss 73.72405 -- Acc: 0.0\n",
      "Loss 78.62832 -- Acc: 0.0\n",
      "Loss 78.9987 -- Acc: 0.0\n",
      "Avg Loss: 17856.60231 -- Avg Acc: 0.00247\n",
      "\n",
      "\n",
      "Beginning Epoch 19\n",
      "Loss 74.29288 -- Acc: 0.0\n",
      "Loss 69.73341 -- Acc: 0.0\n",
      "Loss 77.953 -- Acc: 0.0\n",
      "Loss 82.41158 -- Acc: 0.0\n",
      "Loss 86.53732 -- Acc: 0.0\n",
      "Loss 70.94801 -- Acc: 0.0\n",
      "Loss 78.08694 -- Acc: 0.0\n",
      "Loss 64.64565 -- Acc: 0.0\n",
      "Loss 75.24007 -- Acc: 0.0\n",
      "Loss 74.41841 -- Acc: 0.0\n",
      "Loss 78.31321 -- Acc: 0.0\n",
      "Loss 79.67618 -- Acc: 0.0\n",
      "Loss 83.87358 -- Acc: 0.0\n",
      "Loss 79.07442 -- Acc: 0.0\n",
      "Loss 85.43961 -- Acc: 0.0\n",
      "Loss 82.159 -- Acc: 0.0\n",
      "Loss 69.79129 -- Acc: 0.0\n",
      "Loss 77.53722 -- Acc: 0.0\n",
      "Loss 76.00323 -- Acc: 0.0\n",
      "Loss 72.05125 -- Acc: 0.0\n",
      "Loss 74.5862 -- Acc: 0.0\n",
      "Loss 89.03562 -- Acc: 0.0\n",
      "Loss 82.58392 -- Acc: 0.0\n",
      "Avg Loss: 17682.6769 -- Avg Acc: 0.0\n",
      "\n",
      "\n",
      "Beginning Epoch 20\n",
      "Loss 71.35521 -- Acc: 0.0\n",
      "Loss 78.79469 -- Acc: 0.0\n",
      "Loss 86.60065 -- Acc: 0.0\n",
      "Loss 74.59552 -- Acc: 0.0\n",
      "Loss 74.9567 -- Acc: 0.0\n",
      "Loss 74.13499 -- Acc: 0.0\n",
      "Loss 73.7836 -- Acc: 0.0\n",
      "Loss 72.38307 -- Acc: 0.0\n",
      "Loss 80.98806 -- Acc: 0.0\n",
      "Loss 91.54433 -- Acc: 0.0\n",
      "Loss 79.67822 -- Acc: 0.0\n",
      "Loss 77.19276 -- Acc: 0.0\n",
      "Loss 78.2685 -- Acc: 0.0\n",
      "Loss 80.46008 -- Acc: 0.0\n",
      "Loss 75.60548 -- Acc: 0.0\n",
      "Loss 78.19643 -- Acc: 0.0\n",
      "Loss 81.55309 -- Acc: 0.0\n",
      "Loss 89.31084 -- Acc: 0.0\n",
      "Loss 75.83247 -- Acc: 0.0\n",
      "Loss 75.24307 -- Acc: 0.0\n",
      "Loss 78.75394 -- Acc: 0.0\n",
      "Loss 78.71962 -- Acc: 0.0\n",
      "Loss 78.14757 -- Acc: 0.0\n",
      "Avg Loss: 17765.27537 -- Avg Acc: 0.0\n",
      "\n",
      "\n",
      "Beginning Epoch 21\n",
      "Loss 70.19128 -- Acc: 0.0\n",
      "Loss 94.7104 -- Acc: 0.0\n",
      "Loss 81.40885 -- Acc: 0.0\n",
      "Loss 68.02068 -- Acc: 0.0\n",
      "Loss 70.67604 -- Acc: 0.0\n",
      "Loss 87.90881 -- Acc: 0.0\n",
      "Loss 72.92852 -- Acc: 0.0\n",
      "Loss 69.18238 -- Acc: 0.0\n",
      "Loss 64.57226 -- Acc: 0.0\n",
      "Loss 86.81875 -- Acc: 0.0\n",
      "Loss 79.16527 -- Acc: 0.0\n",
      "Loss 73.09229 -- Acc: 0.0\n",
      "Loss 77.99346 -- Acc: 0.0\n",
      "Loss 79.74513 -- Acc: 0.0\n",
      "Loss 77.08403 -- Acc: 0.0\n",
      "Loss 86.6022 -- Acc: 0.0\n",
      "Loss 82.32137 -- Acc: 0.0\n",
      "Loss 78.14072 -- Acc: 0.0\n",
      "Loss 91.92801 -- Acc: 0.0\n",
      "Loss 76.88219 -- Acc: 0.0\n",
      "Loss 78.7956 -- Acc: 0.0\n",
      "Loss 67.70298 -- Acc: 0.0\n",
      "Loss 84.54861 -- Acc: 0.0\n",
      "Avg Loss: 17870.48734 -- Avg Acc: 0.0\n",
      "\n",
      "\n",
      "Beginning Epoch 22\n",
      "Loss 70.1618 -- Acc: 0.0\n",
      "Loss 68.14333 -- Acc: 0.0\n",
      "Loss 86.80003 -- Acc: 0.0\n",
      "Loss 91.19104 -- Acc: 0.0\n",
      "Loss 75.2286 -- Acc: 0.0\n",
      "Loss 64.27391 -- Acc: 0.0\n",
      "Loss 76.53624 -- Acc: 0.0\n",
      "Loss 72.07988 -- Acc: 0.0\n",
      "Loss 81.77596 -- Acc: 0.0\n",
      "Loss 82.28808 -- Acc: 0.0\n",
      "Loss 75.64957 -- Acc: 0.0\n",
      "Loss 78.31706 -- Acc: 0.0\n",
      "Loss 70.71651 -- Acc: 0.0\n",
      "Loss 64.18954 -- Acc: 0.0\n",
      "Loss 85.0622 -- Acc: 0.0\n",
      "Loss 76.49306 -- Acc: 0.0\n",
      "Loss 75.81965 -- Acc: 0.0\n",
      "Loss 82.79537 -- Acc: 0.0\n",
      "Loss 96.55973 -- Acc: 0.0\n",
      "Loss 79.18594 -- Acc: 0.0\n",
      "Loss 73.47423 -- Acc: 0.0\n",
      "Loss 85.38253 -- Acc: 0.0\n",
      "Loss 78.52869 -- Acc: 0.0\n",
      "Avg Loss: 17902.07182 -- Avg Acc: 0.0\n",
      "\n",
      "\n",
      "Beginning Epoch 23\n",
      "Loss 87.02235 -- Acc: 0.0\n",
      "Loss 86.92616 -- Acc: 0.0\n",
      "Loss 66.01427 -- Acc: 0.0\n",
      "Loss 83.68465 -- Acc: 0.0\n",
      "Loss 74.97939 -- Acc: 0.0\n",
      "Loss 68.78711 -- Acc: 0.0\n",
      "Loss 68.0621 -- Acc: 0.0\n",
      "Loss 72.97118 -- Acc: 0.0\n",
      "Loss 85.28156 -- Acc: 0.0\n",
      "Loss 88.77387 -- Acc: 0.0\n",
      "Loss 101.67509 -- Acc: 0.0\n",
      "Loss 82.31714 -- Acc: 0.0\n",
      "Loss 75.58431 -- Acc: 0.0\n",
      "Loss 81.47666 -- Acc: 0.0\n",
      "Loss 101.99596 -- Acc: 0.0\n",
      "Loss 81.58829 -- Acc: 0.0\n",
      "Loss 72.46165 -- Acc: 0.0\n",
      "Loss 81.86639 -- Acc: 0.0\n",
      "Loss 76.67555 -- Acc: 0.0\n",
      "Loss 82.03859 -- Acc: 0.0\n",
      "Loss 74.24858 -- Acc: 0.0\n",
      "Loss 69.21114 -- Acc: 0.0\n",
      "Loss 83.14462 -- Acc: 0.0\n",
      "Avg Loss: 17890.66956 -- Avg Acc: 0.0\n",
      "\n",
      "\n",
      "Beginning Epoch 24\n",
      "Loss 69.96284 -- Acc: 0.0\n",
      "Loss 78.71507 -- Acc: 0.0\n",
      "Loss 75.34171 -- Acc: 0.0\n",
      "Loss 80.95049 -- Acc: 0.0\n",
      "Loss 83.87508 -- Acc: 0.0\n",
      "Loss 93.10999 -- Acc: 0.0\n",
      "Loss 87.95605 -- Acc: 0.0\n",
      "Loss 76.62834 -- Acc: 0.0\n",
      "Loss 75.12321 -- Acc: 0.0\n",
      "Loss 72.33045 -- Acc: 0.0\n",
      "Loss 78.46934 -- Acc: 0.0\n",
      "Loss 83.86032 -- Acc: 0.0\n",
      "Loss 84.50833 -- Acc: 0.0\n",
      "Loss 71.00498 -- Acc: 0.0\n",
      "Loss 77.73286 -- Acc: 0.0\n",
      "Loss 67.91309 -- Acc: 0.0\n",
      "Loss 85.09583 -- Acc: 0.0\n",
      "Loss 69.4174 -- Acc: 0.0\n",
      "Loss 79.67583 -- Acc: 0.0\n",
      "Loss 83.21203 -- Acc: 0.0\n",
      "Loss 81.87596 -- Acc: 0.0\n",
      "Loss 68.68127 -- Acc: 0.0\n",
      "Loss 84.69115 -- Acc: 0.0\n",
      "Avg Loss: 17769.4165 -- Avg Acc: 0.00228\n",
      "\n",
      "\n",
      "Beginning Epoch 25\n",
      "Loss 61.1929 -- Acc: 0.0\n",
      "Loss 74.99469 -- Acc: 0.0\n",
      "Loss 75.87518 -- Acc: 0.0\n",
      "Loss 72.5443 -- Acc: 0.0\n",
      "Loss 76.38499 -- Acc: 0.0\n",
      "Loss 73.25186 -- Acc: 0.0\n",
      "Loss 82.75381 -- Acc: 0.0\n",
      "Loss 66.17372 -- Acc: 0.0\n",
      "Loss 86.60601 -- Acc: 0.0\n",
      "Loss 82.08041 -- Acc: 0.0\n",
      "Loss 83.96512 -- Acc: 0.0\n",
      "Loss 79.38669 -- Acc: 0.0\n",
      "Loss 85.89906 -- Acc: 0.0\n",
      "Loss 67.36926 -- Acc: 0.0\n",
      "Loss 72.6096 -- Acc: 0.0\n",
      "Loss 77.05771 -- Acc: 0.0\n",
      "Loss 69.53568 -- Acc: 0.0\n",
      "Loss 68.10493 -- Acc: 0.0\n",
      "Loss 83.99649 -- Acc: 0.0\n",
      "Loss 73.33067 -- Acc: 0.0\n",
      "Loss 79.34822 -- Acc: 0.0\n",
      "Loss 83.37138 -- Acc: 0.0\n",
      "Loss 79.96837 -- Acc: 0.0\n",
      "Avg Loss: 18036.51147 -- Avg Acc: 0.0\n",
      "\n",
      "\n",
      "Beginning Epoch 26\n",
      "Loss 79.85292 -- Acc: 0.0\n",
      "Loss 76.08904 -- Acc: 0.0\n",
      "Loss 73.66779 -- Acc: 0.0\n",
      "Loss 83.02142 -- Acc: 0.0\n",
      "Loss 77.38589 -- Acc: 0.0\n",
      "Loss 64.80058 -- Acc: 0.0\n",
      "Loss 70.48492 -- Acc: 0.0\n",
      "Loss 78.02954 -- Acc: 0.0\n",
      "Loss 69.6651 -- Acc: 0.0\n",
      "Loss 76.86335 -- Acc: 0.0\n",
      "Loss 88.75536 -- Acc: 0.0\n",
      "Loss 82.12515 -- Acc: 0.0\n",
      "Loss 80.06484 -- Acc: 0.0\n",
      "Loss 73.71937 -- Acc: 0.0\n",
      "Loss 75.91632 -- Acc: 0.0\n",
      "Loss 85.17828 -- Acc: 0.0\n",
      "Loss 76.21852 -- Acc: 0.0\n",
      "Loss 86.99968 -- Acc: 0.0\n",
      "Loss 82.67375 -- Acc: 0.0\n",
      "Loss 67.96363 -- Acc: 0.0\n",
      "Loss 69.00156 -- Acc: 0.0\n",
      "Loss 89.7635 -- Acc: 0.0\n",
      "Loss 85.87207 -- Acc: 0.0\n",
      "Avg Loss: 17809.97445 -- Avg Acc: 0.0\n",
      "\n",
      "\n",
      "Beginning Epoch 27\n",
      "Loss 78.94858 -- Acc: 0.0\n",
      "Loss 69.16323 -- Acc: 0.0\n",
      "Loss 71.11575 -- Acc: 0.0\n",
      "Loss 82.89697 -- Acc: 0.0\n",
      "Loss 71.79827 -- Acc: 0.0\n",
      "Loss 71.71954 -- Acc: 0.0\n",
      "Loss 77.81748 -- Acc: 0.0\n",
      "Loss 71.26684 -- Acc: 0.0\n",
      "Loss 70.67496 -- Acc: 0.0\n",
      "Loss 74.16792 -- Acc: 0.0\n",
      "Loss 68.34711 -- Acc: 0.0\n",
      "Loss 73.38336 -- Acc: 0.0\n",
      "Loss 71.38365 -- Acc: 0.0\n",
      "Loss 70.71732 -- Acc: 0.0\n",
      "Loss 67.71807 -- Acc: 0.0\n",
      "Loss 77.03152 -- Acc: 0.0\n",
      "Loss 82.45536 -- Acc: 0.0\n",
      "Loss 79.14452 -- Acc: 0.0\n",
      "Loss 72.11543 -- Acc: 0.0\n",
      "Loss 62.53389 -- Acc: 0.0\n",
      "Loss 94.0108 -- Acc: 0.0\n",
      "Loss 87.43855 -- Acc: 0.0\n",
      "Loss 73.28153 -- Acc: 0.0\n",
      "Avg Loss: 17782.17863 -- Avg Acc: 0.01279\n",
      "\n",
      "\n",
      "Beginning Epoch 28\n",
      "Loss 72.2069 -- Acc: 0.0\n",
      "Loss 94.38067 -- Acc: 0.0\n",
      "Loss 83.19879 -- Acc: 0.0\n",
      "Loss 68.77099 -- Acc: 0.0\n",
      "Loss 83.86135 -- Acc: 0.0\n",
      "Loss 74.0724 -- Acc: 0.0\n",
      "Loss 66.31905 -- Acc: 0.0\n",
      "Loss 85.69831 -- Acc: 0.0\n",
      "Loss 76.15824 -- Acc: 0.0\n",
      "Loss 75.50061 -- Acc: 0.0\n",
      "Loss 71.33575 -- Acc: 0.0\n",
      "Loss 76.71121 -- Acc: 0.0\n",
      "Loss 76.1199 -- Acc: 0.0\n",
      "Loss 77.77209 -- Acc: 0.0\n",
      "Loss 82.92622 -- Acc: 0.0\n",
      "Loss 82.27337 -- Acc: 0.0\n",
      "Loss 82.59924 -- Acc: 0.0\n",
      "Loss 74.60432 -- Acc: 0.0\n",
      "Loss 75.30096 -- Acc: 0.0\n",
      "Loss 78.16983 -- Acc: 0.0\n",
      "Loss 76.09438 -- Acc: 0.0\n",
      "Loss 88.10798 -- Acc: 0.0\n",
      "Loss 67.6703 -- Acc: 0.0\n",
      "Avg Loss: 18033.29948 -- Avg Acc: 0.0\n",
      "\n",
      "\n",
      "Beginning Epoch 29\n",
      "Loss 71.65161 -- Acc: 0.0\n",
      "Loss 61.9356 -- Acc: 0.0\n",
      "Loss 82.98694 -- Acc: 0.0\n",
      "Loss 69.59393 -- Acc: 0.0\n",
      "Loss 91.11538 -- Acc: 0.0\n",
      "Loss 75.35097 -- Acc: 0.0\n",
      "Loss 74.10693 -- Acc: 0.0\n",
      "Loss 75.16991 -- Acc: 0.0\n",
      "Loss 75.03696 -- Acc: 0.0\n",
      "Loss 74.06474 -- Acc: 0.0\n",
      "Loss 79.8309 -- Acc: 0.0\n",
      "Loss 79.9417 -- Acc: 0.0\n",
      "Loss 61.76777 -- Acc: 0.0\n",
      "Loss 70.42159 -- Acc: 0.0\n",
      "Loss 64.81308 -- Acc: 0.0\n",
      "Loss 73.63474 -- Acc: 0.0\n",
      "Loss 89.69135 -- Acc: 0.0\n",
      "Loss 77.89735 -- Acc: 0.0\n",
      "Loss 80.62149 -- Acc: 0.0\n",
      "Loss 87.98573 -- Acc: 0.0\n",
      "Loss 83.37015 -- Acc: 0.0\n",
      "Loss 96.76021 -- Acc: 0.0\n",
      "Loss 83.93723 -- Acc: 0.0\n",
      "Avg Loss: 18098.25408 -- Avg Acc: 0.0\n",
      "\n",
      "\n",
      "Beginning Epoch 30\n",
      "Loss 86.86234 -- Acc: 0.0\n",
      "Loss 79.71094 -- Acc: 0.0\n",
      "Loss 91.29276 -- Acc: 0.0\n",
      "Loss 76.61295 -- Acc: 0.0\n",
      "Loss 85.30524 -- Acc: 0.0\n",
      "Loss 75.10213 -- Acc: 0.0\n",
      "Loss 69.08434 -- Acc: 0.0\n",
      "Loss 84.2804 -- Acc: 0.0\n",
      "Loss 73.59651 -- Acc: 0.0\n",
      "Loss 78.40434 -- Acc: 0.0\n",
      "Loss 73.82139 -- Acc: 0.0\n",
      "Loss 71.74115 -- Acc: 0.0\n",
      "Loss 77.82107 -- Acc: 0.0\n",
      "Loss 83.11311 -- Acc: 0.0\n",
      "Loss 81.58401 -- Acc: 0.0\n",
      "Loss 74.11356 -- Acc: 0.0\n",
      "Loss 83.75749 -- Acc: 0.0\n",
      "Loss 84.00463 -- Acc: 0.0\n",
      "Loss 73.25227 -- Acc: 0.0\n",
      "Loss 96.13843 -- Acc: 0.0\n",
      "Loss 70.01058 -- Acc: 0.0\n",
      "Loss 64.32793 -- Acc: 0.0\n",
      "Loss 76.79844 -- Acc: 0.0\n",
      "Avg Loss: 18049.66694 -- Avg Acc: 0.01674\n",
      "\n",
      "\n",
      "Beginning Epoch 31\n",
      "Loss 65.86492 -- Acc: 0.0\n",
      "Loss 76.95194 -- Acc: 0.0\n",
      "Loss 75.05942 -- Acc: 0.0\n",
      "Loss 80.86308 -- Acc: 0.0\n",
      "Loss 77.78259 -- Acc: 0.0\n",
      "Loss 81.53475 -- Acc: 0.0\n",
      "Loss 75.56462 -- Acc: 0.0\n",
      "Loss 70.26108 -- Acc: 0.0\n",
      "Loss 78.82692 -- Acc: 0.0\n",
      "Loss 91.21783 -- Acc: 0.0\n",
      "Loss 87.71928 -- Acc: 0.0\n",
      "Loss 73.31205 -- Acc: 0.0\n",
      "Loss 82.8728 -- Acc: 0.0\n",
      "Loss 75.4365 -- Acc: 0.0\n",
      "Loss 69.28349 -- Acc: 0.0\n",
      "Loss 80.88274 -- Acc: 0.0\n",
      "Loss 75.13544 -- Acc: 0.0\n",
      "Loss 74.09536 -- Acc: 0.0\n",
      "Loss 81.49937 -- Acc: 0.0\n",
      "Loss 71.56938 -- Acc: 0.0\n",
      "Loss 77.48453 -- Acc: 0.0\n",
      "Loss 83.81333 -- Acc: 0.0\n",
      "Loss 80.88471 -- Acc: 0.0\n",
      "Avg Loss: 17691.65824 -- Avg Acc: 0.0\n",
      "\n",
      "\n",
      "Beginning Epoch 32\n",
      "Loss 80.79836 -- Acc: 0.0\n",
      "Loss 72.15221 -- Acc: 0.0\n",
      "Loss 78.78267 -- Acc: 0.0\n",
      "Loss 90.37373 -- Acc: 0.0\n",
      "Loss 75.48962 -- Acc: 0.0\n",
      "Loss 87.94167 -- Acc: 0.0\n",
      "Loss 92.07954 -- Acc: 0.0\n",
      "Loss 79.27767 -- Acc: 0.0\n",
      "Loss 77.65549 -- Acc: 0.0\n",
      "Loss 75.47416 -- Acc: 0.0\n",
      "Loss 75.72551 -- Acc: 0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_503145/2415796064.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"output_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_503145/2159497757.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout_embs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1041\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1043\u001b[0;31m         transformer_outputs = self.transformer(\n\u001b[0m\u001b[1;32m   1044\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m             \u001b[0mpast_key_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    885\u001b[0m                 )\n\u001b[1;32m    886\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 887\u001b[0;31m                 outputs = block(\n\u001b[0m\u001b[1;32m    888\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m                     \u001b[0mlayer_past\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_past\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m         attn_outputs = self.attn(\n\u001b[0m\u001b[1;32m    389\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m             \u001b[0mlayer_past\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_past\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mattn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_upcast_and_reordered_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m             \u001b[0mattn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_merge_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36m_attn\u001b[0;34m(self, query, key, value, attention_mask, head_mask)\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0;31m# Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar type float but found double`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0;31m# Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m             \u001b[0mmask_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m             \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.cuda()\n",
    "for epoch in range(n_epochs):\n",
    "    print()\n",
    "    print(\"Beginning Epoch\", epoch)\n",
    "    model.train()\n",
    "    avg_loss = 0\n",
    "    avg_acc = 0\n",
    "    for i,data in enumerate(dataloader):\n",
    "        data = {k: v.cuda() for k,v in data.items()}\n",
    "        preds = model(data)\n",
    "        preds = preds[:,:-1].reshape(-1, preds.shape[-1])\n",
    "        labels = data[\"output_ids\"].reshape(-1)\n",
    "        idx = labels!=tokenizer.pad_token_id\n",
    "        loss = loss_fxn(preds[idx], labels[idx])\n",
    "        avg_loss += loss.item()\n",
    "        argmax = torch.argmax(preds[idx])\n",
    "        acc = (argmax==labels[idx]).float().mean()\n",
    "        avg_acc += acc.item()\n",
    "        if i%10==0: print(\"Loss\", round(loss.item(), 5), \"-- Acc:\", round(acc.item(), 5))\n",
    "    print(\"Avg Loss:\", round(avg_loss, 5), \"-- Avg Acc:\", round(avg_acc, 5))\n",
    "    print()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bb16e4-960e-4db7-a501-31edc66827cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ef7cf5-09dc-45e2-bbf7-065b6b086e84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85eaf26-cfee-4795-98a5-06cd08f96d4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
